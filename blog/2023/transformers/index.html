<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
      }
    });
    </script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Transformers | Mohammad Arshad</title> <meta name="author" content="Mohammad Arshad"> <meta name="description" content="What are Transformers?"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mdarshad1000.github.io/blog/2023/transformers/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Mohammad </span>Arshad</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/experiences/">experiences</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/awards/">awards</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Transformers</h1> <p class="post-meta">March 15, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h3 id="transformers">Transformers</h3> <p><code class="language-plaintext highlighter-rouge">What are Transformers?</code></p> <p>Transformers are a type of neural network architecture that was introduced in the paper “Attention is All You Need” by Vaswani et al. in 2017. Since then, it has become one of the most popular and successful models in natural language processing (NLP) tasks such as language translation, summarization, and text classification. Furthermore it is the foundation for Language Models and their application.</p> <p>The key innovation of the Transformer architecture is the use of attention mechanisms. In a traditional neural network, each input is processed independently, without any information about the relationships between the inputs. In contrast, the Transformer model uses attention mechanisms to weight the inputs based on their relevance to the output.</p> <p>For example, in a language translation task, the Transformer model might pay more attention to the words at the beginning and end of a sentence, as these are typically more important for determining the overall meaning of the sentence. On the other hand, it might pay less attention to words that are less important or less relevant to the translation.</p> <p>Another key advantage of the Transformer architecture is its ability to process input sequences in parallel. Traditional recurrent neural networks (RNNs), which are commonly used in NLP tasks, process input sequences one element at a time, making them slow and inefficient. In contrast, the Transformer model processes all elements of the input sequence at the same time, allowing it to run much faster and more efficiently.</p> <p>Attention At a high level, the Transformer model is based on the idea of self-attention, which allows the model to weight the input elements based on their relevance to the output. Mathematically, self-attention can be computed using the following formula.</p> <h4><i>$$Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$</i></h4> <p>Here, \(Q\),\(K\), and \(V\) are matrices of query, key, and value vectors, respectively. \(d_ k\)​ is the dimensionality of the key vectors. The dot product of \(Q\) and \(K^T\) is divided by the square root of \(d_k\)​ to ensure that the dot products do not become too large and blow up the softmax function. The output of the self-attention layer is a weighted sum of the value vectors, with the weights determined by the dot products of the query and key vectors. In addition to self-attention, the Transformer model also includes feed-forward layers and residual connections. The feed-forward layers consist of a linear transformation followed by a non-linear activation function, such as ReLU. The output of the feed-forward layers is then added to the output of the self-attention layers using residual connections.</p> <p>Overall, the Transformer model can be described using the following pseudo-code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">each</span> <span class="nb">input</span> <span class="n">sequence</span><span class="p">:</span>
<span class="n">encode</span> <span class="nb">input</span> <span class="n">sequence</span> <span class="n">using</span> <span class="n">self</span><span class="o">-</span><span class="n">attention</span> <span class="ow">and</span> <span class="n">feed</span><span class="o">-</span><span class="n">forward</span> <span class="n">layers</span>
<span class="n">add</span> <span class="n">the</span> <span class="n">output</span> <span class="n">to</span> <span class="n">the</span> <span class="n">original</span> <span class="nb">input</span> <span class="n">using</span> <span class="n">residual</span> <span class="n">connections</span>
<span class="nb">apply</span> <span class="n">layer</span> <span class="n">normalization</span>

<span class="nb">apply</span> <span class="n">final</span> <span class="n">self</span><span class="o">-</span><span class="n">attention</span> <span class="ow">and</span> <span class="n">feed</span><span class="o">-</span><span class="n">forward</span> <span class="n">layers</span> <span class="n">to</span> <span class="n">obtain</span> <span class="n">output</span>

</code></pre></div></div> <h2>How to set up a Transformer Model?</h2> <p>Here is an example of how you can set up a Transformer model in Python using the <code class="language-plaintext highlighter-rouge">Transformers</code> library:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">transformers</span>

<span class="c1"># Set up the Transformer model and tokenizer
</span><span class="n">model_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">bert-base-cased</span><span class="sh">'</span> <span class="c1"># choose a pre-trained model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="n">BertModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="n">BertTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># Tokenize input text
</span><span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">This is some input text that I want to feed into the Transformer model.</span><span class="sh">"</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">'</span><span class="s">pt</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># convert text to numerical input
</span>
<span class="c1"># Run input through the model
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
</code></pre></div></div> <p>This code sets up a BertModel from the Transformers library, which is a type of Transformer model developed by Google. It also sets up a BertTokenizer, which is used to convert the input text into a numerical representation that can be fed into the model.</p> <p>Finally, the code tokenizes the input text and passes it through the model to obtain the output. The output of the model will be a tensor containing the encoded representation of the input text.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/doctagging/">Tagging Document - LangChain</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/llamindexvlangchain/">LangChain vs (and?) LlamaIndex 🦜🦙</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/chatgpt/">ChatGPT</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/datasheet/">Navigating the Data Jungle</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/langchain/">LangChain 🦜</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Mohammad Arshad. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>